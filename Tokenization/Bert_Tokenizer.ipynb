{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbllr0KyofGw",
        "outputId": "8ba1992f-b7d6-4d10-fd50-5ff045f899c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detecting hate speech in Twitter posts**"
      ],
      "metadata": {
        "id": "xupgz6aIj7X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing\n"
      ],
      "metadata": {
        "id": "T_UZh5gWkYPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import emoji\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/labeled_dataset.csv')\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7OP6Sm09ptpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f107c2b-654a-4dea-b5e8-4e9a6ffaf67c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary for message short forms and slangs\n",
        "abbreviations = {\n",
        "    \"$\": \"dollar\",\n",
        "    \"€\": \"euro\",\n",
        "    \"4ao\": \"for adults only\",\n",
        "    \"a.m\": \"before midday\",\n",
        "    \"a3\": \"anytime anywhere anyplace\",\n",
        "    \"aamof\": \"as a matter of fact\",\n",
        "    \"acct\": \"account\",\n",
        "    \"adih\": \"another day in hell\",\n",
        "    \"afaic\": \"as far as i am concerned\",\n",
        "    \"afaict\": \"as far as i can tell\",\n",
        "    \"afaik\": \"as far as i know\",\n",
        "    \"afair\": \"as far as i remember\",\n",
        "    \"afk\": \"away from keyboard\",\n",
        "    \"app\": \"application\",\n",
        "    \"approx\": \"approximately\",\n",
        "    \"apps\": \"applications\",\n",
        "    \"asap\": \"as soon as possible\",\n",
        "    \"asl\": \"age sex location\",\n",
        "    \"atk\": \"at the keyboard\",\n",
        "    \"ave.\": \"avenue\",\n",
        "    \"aymm\": \"are you my mother\",\n",
        "    \"ayor\": \"at your own risk\",\n",
        "    \"b&b\": \"bed and breakfast\",\n",
        "    \"b+b\": \"bed and breakfast\",\n",
        "    \"b.c\": \"before christ\",\n",
        "    \"b2b\": \"business to business\",\n",
        "    \"b2c\": \"business to customer\",\n",
        "    \"b4\": \"before\",\n",
        "    \"b4n\": \"bye for now\",\n",
        "    \"b@u\": \"back at you\",\n",
        "    \"bae\": \"before anyone else\",\n",
        "    \"bak\": \"back at keyboard\",\n",
        "    \"bbbg\": \"bye bye be good\",\n",
        "    \"bbc\": \"british broadcasting corporation\",\n",
        "    \"bbias\": \"be back in a second\",\n",
        "    \"bbl\": \"be back later\",\n",
        "    \"bbs\": \"be back soon\",\n",
        "    \"be4\": \"before\",\n",
        "    \"bfn\": \"bye for now\",\n",
        "    \"blvd\": \"boulevard\",\n",
        "    \"bout\": \"about\",\n",
        "    \"brb\": \"be right back\",\n",
        "    \"bros\": \"brothers\",\n",
        "    \"brt\": \"be right there\",\n",
        "    \"bsaaw\": \"big smile and a wink\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"bwl\": \"bursting with laughter\",\n",
        "    \"c/o\": \"care of\",\n",
        "    \"cet\": \"central european time\",\n",
        "    \"cf\": \"compare\",\n",
        "    \"cia\": \"central intelligence agency\",\n",
        "    \"csl\": \"cannot stop laughing\",\n",
        "    \"cu\": \"see you\",\n",
        "    \"cul8r\": \"see you later\",\n",
        "    \"cv\": \"curriculum vitae\",\n",
        "    \"cwot\": \"complete waste of time\",\n",
        "    \"cya\": \"see you\",\n",
        "    \"cyt\": \"see you tomorrow\",\n",
        "    \"dae\": \"does anyone else\",\n",
        "    \"dbmib\": \"do not bother me i am busy\",\n",
        "    \"diy\": \"do it yourself\",\n",
        "    \"dm\": \"direct message\",\n",
        "    \"dwh\": \"during work hours\",\n",
        "    \"e123\": \"easy as one two three\",\n",
        "    \"eet\": \"eastern european time\",\n",
        "    \"eg\": \"example\",\n",
        "    \"embm\": \"early morning business meeting\",\n",
        "    \"encl\": \"enclosed\",\n",
        "    \"encl.\": \"enclosed\",\n",
        "    \"etc\": \"and so on\",\n",
        "    \"faq\": \"frequently asked questions\",\n",
        "    \"fawc\": \"for anyone who cares\",\n",
        "    \"fb\": \"facebook\",\n",
        "    \"fc\": \"fingers crossed\",\n",
        "    \"fig\": \"figure\",\n",
        "    \"fimh\": \"forever in my heart\",\n",
        "    \"ft.\": \"feet\",\n",
        "    \"ft\": \"featuring\",\n",
        "    \"ftl\": \"for the loss\",\n",
        "    \"ftw\": \"for the win\",\n",
        "    \"fwiw\": \"for what it is worth\",\n",
        "    \"fyi\": \"for your information\",\n",
        "    \"g9\": \"genius\",\n",
        "    \"gahoy\": \"get a hold of yourself\",\n",
        "    \"gal\": \"get a life\",\n",
        "    \"gcse\": \"general certificate of secondary education\",\n",
        "    \"gfn\": \"gone for now\",\n",
        "    \"gg\": \"good game\",\n",
        "    \"gl\": \"good luck\",\n",
        "    \"glhf\": \"good luck have fun\",\n",
        "    \"gmt\": \"greenwich mean time\",\n",
        "    \"gmta\": \"great minds think alike\",\n",
        "    \"gn\": \"good night\",\n",
        "    \"g.o.a.t\": \"greatest of all time\",\n",
        "    \"goat\": \"greatest of all time\",\n",
        "    \"goi\": \"get over it\",\n",
        "    \"gps\": \"global positioning system\",\n",
        "    \"gr8\": \"great\",\n",
        "    \"gratz\": \"congratulations\",\n",
        "    \"gyal\": \"girl\",\n",
        "    \"h&c\": \"hot and cold\",\n",
        "    \"hp\": \"horsepower\",\n",
        "    \"hr\": \"hour\",\n",
        "    \"hrh\": \"his royal highness\",\n",
        "    \"ht\": \"height\",\n",
        "    \"ibrb\": \"i will be right back\",\n",
        "    \"ic\": \"i see\",\n",
        "    \"icq\": \"i seek you\",\n",
        "    \"icymi\": \"in case you missed it\",\n",
        "    \"idc\": \"i do not care\",\n",
        "    \"idgadf\": \"i do not give a damn fuck\",\n",
        "    \"idgaf\": \"i do not give a fuck\",\n",
        "    \"idk\": \"i do not know\",\n",
        "    \"ie\": \"that is\",\n",
        "    \"i.e\": \"that is\",\n",
        "    \"ig\":\"i guess\",\n",
        "    \"ifyp\": \"i feel your pain\",\n",
        "    \"IG\": \"instagram\",\n",
        "    \"iirc\": \"if i remember correctly\",\n",
        "    \"ilu\": \"i love you\",\n",
        "    \"ily\": \"i love you\",\n",
        "    \"imho\": \"in my humble opinion\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"imu\": \"i miss you\",\n",
        "    \"iow\": \"in other words\",\n",
        "    \"irl\": \"in real life\",\n",
        "    \"j4f\": \"just for fun\",\n",
        "    \"jic\": \"just in case\",\n",
        "    \"jk\": \"just kidding\",\n",
        "    \"jsyk\": \"just so you know\",\n",
        "    \"l8r\": \"later\",\n",
        "    \"lb\": \"pound\",\n",
        "    \"lbs\": \"pounds\",\n",
        "    \"ldr\": \"long distance relationship\",\n",
        "    \"lmao\": \"laugh my ass off\",\n",
        "    \"lmfao\": \"laugh my fucking ass off\",\n",
        "    \"lol\": \"laughing out loud\",\n",
        "    \"ltd\": \"limited\",\n",
        "    \"ltns\": \"long time no see\",\n",
        "    \"m8\": \"mate\",\n",
        "    \"mf\": \"motherfucker\",\n",
        "    \"mfs\": \"motherfuckers\",\n",
        "    \"mfw\": \"my face when\",\n",
        "    \"mofo\": \"motherfucker\",\n",
        "    \"mph\": \"miles per hour\",\n",
        "    \"mr\": \"mister\",\n",
        "    \"mrw\": \"my reaction when\",\n",
        "    \"ms\": \"miss\",\n",
        "    \"mte\": \"my thoughts exactly\",\n",
        "    \"nagi\": \"not a good idea\",\n",
        "    \"nbc\": \"national broadcasting company\",\n",
        "    \"nbd\": \"not big deal\",\n",
        "    \"nfs\": \"not for sale\",\n",
        "    \"ngl\": \"not going to lie\",\n",
        "    \"nhs\": \"national health service\",\n",
        "    \"nrn\": \"no reply necessary\",\n",
        "    \"nsfl\": \"not safe for life\",\n",
        "    \"nsfw\": \"not safe for work\",\n",
        "    \"nth\": \"nice to have\",\n",
        "    \"nvr\": \"never\",\n",
        "    \"nyc\": \"new york city\",\n",
        "    \"oc\": \"original content\",\n",
        "    \"og\": \"original\",\n",
        "    \"ohp\": \"overhead projector\",\n",
        "    \"oic\": \"oh i see\",\n",
        "    \"omdb\": \"over my dead body\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"omw\": \"on my way\",\n",
        "    \"p.a\": \"per annum\",\n",
        "    \"plz\":\"please\",\n",
        "    \"p.m\": \"after midday\",\n",
        "    \"pm\": \"prime minister\",\n",
        "    \"poc\": \"people of color\",\n",
        "    \"pov\": \"point of view\",\n",
        "    \"pp\": \"pages\",\n",
        "    \"ppl\": \"people\",\n",
        "    \"prw\": \"parents are watching\",\n",
        "    \"ps\": \"postscript\",\n",
        "    \"pt\": \"point\",\n",
        "    \"ptb\": \"please text back\",\n",
        "    \"pto\": \"please turn over\",\n",
        "    \"qpsa\": \"what happens\",  # \"que pasa\",\n",
        "    \"ratchet\": \"rude\",\n",
        "    \"rbtl\": \"read between the lines\",\n",
        "    \"rlrt\": \"real life retweet\",\n",
        "    \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"roflol\": \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\": \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\": \"retweet\",\n",
        "    \"rt\":\"retweet\",\n",
        "    \"ru\": \"are you\",\n",
        "    \"shid\": \"slaps head in disgust\",\n",
        "    \"somy\": \"sick of me yet\",\n",
        "    \"ruok\": \"are you ok\",\n",
        "    \"sfw\": \"safe for work\",\n",
        "    \"sk8\": \"skate\",\n",
        "    \"smh\": \"shake my head\",\n",
        "    \"sq\": \"square\",\n",
        "    \"srsly\": \"seriously\",\n",
        "    \"ssdd\": \"same stuff different day\",\n",
        "    \"tbh\": \"to be honest\",\n",
        "    \"tbs\": \"tablespoonful\",\n",
        "    \"tbsp\": \"tablespoonful\",\n",
        "    \"tfw\": \"that feeling when\",\n",
        "    \"thks\": \"thank you\",\n",
        "    \"tho\": \"though\",\n",
        "    \"thx\": \"thank you\",\n",
        "    \"tia\": \"thanks in advance\",\n",
        "    \"til\": \"today i learned\",\n",
        "    \"tl;dr\": \"too long i did not read\",\n",
        "    \"tldr\": \"too long i did not read\",\n",
        "    \"tmb\": \"tweet me back\",\n",
        "    \"tntl\": \"trying not to laugh\",\n",
        "    \"ttyl\": \"talk to you later\",\n",
        "    \"u\": \"you\",\n",
        "    \"u2\": \"you too\",\n",
        "    \"u4e\": \"yours forever\",\n",
        "    \"utc\": \"coordinated universal time\",\n",
        "    \"w/\": \"with\",\n",
        "    \"w/o\": \"without\",\n",
        "    \"w8\": \"wait\",\n",
        "    \"wassup\": \"what is up\",\n",
        "    \"wb\": \"welcome back\",\n",
        "    \"wtf\": \"what the fuck\",\n",
        "    \"wtg\": \"way to go\",\n",
        "    \"wtpa\": \"where the party at\",\n",
        "    \"wuf\": \"where are you from\",\n",
        "    \"wuzup\": \"what is up\",\n",
        "    \"wywh\": \"wish you were here\",\n",
        "    \"yd\": \"yard\",\n",
        "    \"u\":\"you\",\n",
        "    \"ygtr\": \"you got that right\",\n",
        "    \"ynk\": \"you never know\",\n",
        "    \"zzz\": \"sleeping bored and tired\"\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vc85DIUzql9H"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to replace abbreviations in text\n",
        "def expand_abbreviations(text):\n",
        "    words = text.split()\n",
        "    expanded_words = [abbreviations[word.lower()] if word.lower() in abbreviations else word for word in words]\n",
        "    return ' '.join(expanded_words)\n",
        "\n",
        "# Define the remove_stop_words function\n",
        "def remove_stop_words(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Function to decode HTML entities\n",
        "def decode_html_entities(text):\n",
        "    return html.unescape(text)\n",
        "\n",
        "# Example function to expand contractions\n",
        "def expand_contractions(text):\n",
        "   expanded_text = contractions.fix(text)\n",
        "   return expanded_text\n",
        "\n",
        "\n",
        "# Function to convert emojis to words\n",
        "def convert_emojis_to_words(text):\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    # Ensure there is a space before and after each emoji word\n",
        "    text = re.sub(r'(:\\w+:)', r' \\1 ', text)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "gUpsLblzq5rz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import html"
      ],
      "metadata": {
        "id": "vlJp-FMsCEbU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['tweet'] = df['tweet'].apply(convert_emojis_to_words)\n",
        "# Normalize text\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = decode_html_entities(text)\n",
        "    text = convert_emojis_to_words(text)\n",
        "    text = expand_abbreviations(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'&amp;', 'and', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(normalize_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2CT-nW6ZrAqW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "df['tweet_tokens'] = df['tweet'].apply(lambda x: tokenizer.tokenize(x))\n"
      ],
      "metadata": {
        "id": "pxPEfLAaBj_P"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['tweet_tokens'])"
      ],
      "metadata": {
        "id": "Puzz-65jBgdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "985fc7d9-0325-4d3e-b5ba-c7f997bc3113"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        [re, ##t, ##we, ##t, as, a, woman, you, should...\n",
            "1        [re, ##t, ##we, ##t, boy, dat, ##s, cold, ##ty...\n",
            "2        [re, ##t, ##we, ##t, da, ##wg, re, ##t, ##we, ...\n",
            "3        [re, ##t, ##we, ##t, she, lok, like, a, tran, ...\n",
            "4        [re, ##t, ##we, ##t, the, shit, you, hear, abo...\n",
            "                               ...                        \n",
            "24778    [you, ##s, a, mu, ##tha, ##fin, lie, right, hi...\n",
            "24779    [you, have, gone, and, broke, the, wrong, hear...\n",
            "24780    [young, buck, want, to, eat, dat, ni, ##gu, ##...\n",
            "24781    [you, got, wild, bitch, ##es, tel, ##in, you, ...\n",
            "24782    [ru, ##fle, ##d, nt, ##ac, e, ##ile, ##n, dahl...\n",
            "Name: tweet_tokens, Length: 24783, dtype: object\n"
          ]
        }
      ]
    }
  ]
}