{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E34gw7DD2W5",
        "outputId": "3f6fe8e0-0d4c-4395-eee0-07e931e54983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "   hate_speech  offensive_language  neither  class  \\\n",
            "0            0                   0        3      1   \n",
            "1            0                   3        0      0   \n",
            "2            0                   3        0      0   \n",
            "3            0                   2        1      0   \n",
            "4            0                   6        0      0   \n",
            "\n",
            "                                               tweet  \\\n",
            "0  rt as a woman you should not complain about cl...   \n",
            "1  rt boy dats coldtyga dwn bad for cufin dat hoe...   \n",
            "2  rt dawg rt you ever fuck a bitch and she start...   \n",
            "3                            rt she lok like a trany   \n",
            "4  rt the shit you hear about me might be true or...   \n",
            "\n",
            "                                        tweet_tokens  \\\n",
            "0  ['rt', 'woman', 'complain', 'cleaning', 'house...   \n",
            "1  ['rt', 'boy', 'dat', 'coldtyga', 'dwn', 'bad',...   \n",
            "2  ['rt', 'dawg', 'rt', 'ever', 'fuck', 'bitch', ...   \n",
            "3                     ['rt', 'lok', 'like', 'trany']   \n",
            "4  ['rt', 'shit', 'hear', 'might', 'true', 'might...   \n",
            "\n",
            "                                        merged_tweet  \n",
            "0  rt woman complain cleaning house man always ta...  \n",
            "1  rt boy dat coldtyga dwn bad cufin dat hoe st p...  \n",
            "2  rt dawg rt ever fuck bitch start cry confused ...  \n",
            "3                                  rt lok like trany  \n",
            "4  rt shit hear might true might faker bitch told ya  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/cleaned_dataset_combined (4).csv')\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(\"Original DataFrame:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "# Convert string representation of list to actual list\n",
        "df['tweet_tokens'] = df['tweet_tokens'].apply(ast.literal_eval)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(\"\\nDataFrame after converting tweet_tokens to lists:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY6Th98SD9-P",
        "outputId": "bcc18ec1-2f3c-4752-a779-5c04bd3e7b93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after converting tweet_tokens to lists:\n",
            "   hate_speech  offensive_language  neither  class  \\\n",
            "0            0                   0        3      1   \n",
            "1            0                   3        0      0   \n",
            "2            0                   3        0      0   \n",
            "3            0                   2        1      0   \n",
            "4            0                   6        0      0   \n",
            "\n",
            "                                               tweet  \\\n",
            "0  rt as a woman you should not complain about cl...   \n",
            "1  rt boy dats coldtyga dwn bad for cufin dat hoe...   \n",
            "2  rt dawg rt you ever fuck a bitch and she start...   \n",
            "3                            rt she lok like a trany   \n",
            "4  rt the shit you hear about me might be true or...   \n",
            "\n",
            "                                        tweet_tokens  \\\n",
            "0  [rt, woman, complain, cleaning, house, man, al...   \n",
            "1  [rt, boy, dat, coldtyga, dwn, bad, cufin, dat,...   \n",
            "2  [rt, dawg, rt, ever, fuck, bitch, start, cry, ...   \n",
            "3                             [rt, lok, like, trany]   \n",
            "4  [rt, shit, hear, might, true, might, faker, bi...   \n",
            "\n",
            "                                        merged_tweet  \n",
            "0  rt woman complain cleaning house man always ta...  \n",
            "1  rt boy dat coldtyga dwn bad cufin dat hoe st p...  \n",
            "2  rt dawg rt ever fuck bitch start cry confused ...  \n",
            "3                                  rt lok like trany  \n",
            "4  rt shit hear might true might faker bitch told ya  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define feature and target variables\n",
        "X = df[['tweet_tokens']]\n",
        "y = df['class']"
      ],
      "metadata": {
        "id": "iY40yuMCD_sn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Define a function to join tokens into a single string\n",
        "X['tweet_tokens'] = X['tweet_tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Use TfidfVectorizer for the 'tweet_tokens' column\n",
        "column_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('tweet_tokens', TfidfVectorizer(), 'tweet_tokens')\n",
        "    ],\n",
        "    remainder='passthrough'  # This keeps the other columns as is\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v84rP9kNEBxk",
        "outputId": "16b3715a-0b50-49de-cee9-eab8e328d89b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-93c6994b220a>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['tweet_tokens'] = X['tweet_tokens'].apply(lambda x: ' '.join(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a Logistic Regression classifier pipeline\n",
        "pipeline = make_pipeline(column_transformer, LogisticRegression(max_iter=1000))\n"
      ],
      "metadata": {
        "id": "aU6unB6mFJ4k"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display the training data\n",
        "print(\"\\nTraining Data:\")\n",
        "print(X_train.head())\n",
        "print(y_train.head())\n",
        "\n",
        "# Display the testing data\n",
        "print(\"\\nTesting Data:\")\n",
        "print(X_test.head())\n",
        "print(y_test.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MO2ldrpEHSu",
        "outputId": "79fc36ca-f738-497a-b230-ca116abc8988"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Data:\n",
            "                                            tweet_tokens\n",
            "9132                             folow nica ig imyungjay\n",
            "22834          people fagot like born way big douche bag\n",
            "18379  rt nigas try act hard realy bitch made facewit...\n",
            "2498                                            bitch ad\n",
            "21131                                  stayin late watch\n",
            "9132     0\n",
            "22834    0\n",
            "18379    0\n",
            "2498     0\n",
            "21131    1\n",
            "Name: class, dtype: int64\n",
            "\n",
            "Testing Data:\n",
            "                                            tweet_tokens\n",
            "10676  lose ovo folowers every year bitch unfolowed h...\n",
            "11644                      moma sister godbye bitch time\n",
            "5227   lol da reason canot talk bout da present cuz n...\n",
            "17257  rt getin hand tated dumb yo bitch leave da bus...\n",
            "14891  rt al hoe going like tonight facewithtearsofjo...\n",
            "10676    0\n",
            "11644    0\n",
            "5227     0\n",
            "17257    0\n",
            "14891    0\n",
            "Name: class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "# Evaluate the model\n",
        "accuracy = pipeline.score(X_test, y_test)\n",
        "print(f'\\nAccuracy: {accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k5FivTrEHWp",
        "outputId": "c3deb29a-eda7-45bd-e0b4-3c5d7cb73a6d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.9440855874041179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the test set results\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Wb-wMcXrEfJ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c17298-60e0-4162-ed28-425dd30a07c1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97      4144\n",
            "           1       0.88      0.77      0.82       810\n",
            "\n",
            "    accuracy                           0.94      4954\n",
            "   macro avg       0.92      0.87      0.89      4954\n",
            "weighted avg       0.94      0.94      0.94      4954\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1F39hvFy42LW"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}