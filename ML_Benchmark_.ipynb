{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Text Preprocessing**"
      ],
      "metadata": {
        "id": "q25cAQOL8-xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install contractions\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import emoji\n",
        "import html\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2La7NlFy9WWV",
        "outputId": "6d4c6f3b-0724-42ee-cdc1-69e2433dd1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/labeled_dataset.csv')\n",
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display the shape of the dataset\n",
        "print(\"Shape of the dataset:\")\n",
        "print(df.shape)\n",
        "\n",
        "# Provide summary statistics for numerical columns\n",
        "print(\"\\nSummary statistics for numerical columns:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSJ25F_C9b_I",
        "outputId": "ddf1d42c-77d4-47ad-b77e-af1fb44cc992"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n",
            "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
            "0           0      3            0                   0        3      2   \n",
            "1           1      3            0                   3        0      1   \n",
            "2           2      3            0                   3        0      1   \n",
            "3           3      3            0                   2        1      1   \n",
            "4           4      6            0                   6        0      1   \n",
            "\n",
            "                                               tweet  \n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
            "Shape of the dataset:\n",
            "(24783, 7)\n",
            "\n",
            "Summary statistics for numerical columns:\n",
            "         Unnamed: 0         count   hate_speech  offensive_language  \\\n",
            "count  24783.000000  24783.000000  24783.000000        24783.000000   \n",
            "mean   12681.192027      3.243473      0.280515            2.413711   \n",
            "std     7299.553863      0.883060      0.631851            1.399459   \n",
            "min        0.000000      3.000000      0.000000            0.000000   \n",
            "25%     6372.500000      3.000000      0.000000            2.000000   \n",
            "50%    12703.000000      3.000000      0.000000            3.000000   \n",
            "75%    18995.500000      3.000000      0.000000            3.000000   \n",
            "max    25296.000000      9.000000      7.000000            9.000000   \n",
            "\n",
            "            neither         class  \n",
            "count  24783.000000  24783.000000  \n",
            "mean       0.549247      1.110277  \n",
            "std        1.113299      0.462089  \n",
            "min        0.000000      0.000000  \n",
            "25%        0.000000      1.000000  \n",
            "50%        0.000000      1.000000  \n",
            "75%        0.000000      1.000000  \n",
            "max        9.000000      2.000000  \n",
            "\n",
            "Missing values in each column:\n",
            "Unnamed: 0            0\n",
            "count                 0\n",
            "hate_speech           0\n",
            "offensive_language    0\n",
            "neither               0\n",
            "class                 0\n",
            "tweet                 0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['tweet'])\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values in each column:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcGFEXTL9-lQ",
        "outputId": "9192807d-b6de-4572-b5bc-1932ade61287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values in each column:\n",
            "Unnamed: 0            0\n",
            "count                 0\n",
            "hate_speech           0\n",
            "offensive_language    0\n",
            "neither               0\n",
            "class                 0\n",
            "tweet                 0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary for message short forms and slangs\n",
        "abbreviations = {\n",
        "    \"$\": \"dollar\",\n",
        "    \"€\": \"euro\",\n",
        "    \"4ao\": \"for adults only\",\n",
        "    \"a.m\": \"before midday\",\n",
        "    \"a3\": \"anytime anywhere anyplace\",\n",
        "    \"aamof\": \"as a matter of fact\",\n",
        "    \"acct\": \"account\",\n",
        "    \"adih\": \"another day in hell\",\n",
        "    \"afaic\": \"as far as i am concerned\",\n",
        "    \"afaict\": \"as far as i can tell\",\n",
        "    \"afaik\": \"as far as i know\",\n",
        "    \"afair\": \"as far as i remember\",\n",
        "    \"afk\": \"away from keyboard\",\n",
        "    \"app\": \"application\",\n",
        "    \"approx\": \"approximately\",\n",
        "    \"apps\": \"applications\",\n",
        "    \"asap\": \"as soon as possible\",\n",
        "    \"asl\": \"age sex location\",\n",
        "    \"atk\": \"at the keyboard\",\n",
        "    \"ave.\": \"avenue\",\n",
        "    \"aymm\": \"are you my mother\",\n",
        "    \"ayor\": \"at your own risk\",\n",
        "    \"b&b\": \"bed and breakfast\",\n",
        "    \"b+b\": \"bed and breakfast\",\n",
        "    \"b.c\": \"before christ\",\n",
        "    \"b2b\": \"business to business\",\n",
        "    \"b2c\": \"business to customer\",\n",
        "    \"b4\": \"before\",\n",
        "    \"b4n\": \"bye for now\",\n",
        "    \"b@u\": \"back at you\",\n",
        "    \"bae\": \"before anyone else\",\n",
        "    \"bak\": \"back at keyboard\",\n",
        "    \"bbbg\": \"bye bye be good\",\n",
        "    \"bbc\": \"british broadcasting corporation\",\n",
        "    \"bbias\": \"be back in a second\",\n",
        "    \"bbl\": \"be back later\",\n",
        "    \"bbs\": \"be back soon\",\n",
        "    \"be4\": \"before\",\n",
        "    \"bfn\": \"bye for now\",\n",
        "    \"blvd\": \"boulevard\",\n",
        "    \"bout\": \"about\",\n",
        "    \"brb\": \"be right back\",\n",
        "    \"bros\": \"brothers\",\n",
        "    \"brt\": \"be right there\",\n",
        "    \"bsaaw\": \"big smile and a wink\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"bwl\": \"bursting with laughter\",\n",
        "    \"c/o\": \"care of\",\n",
        "    \"cet\": \"central european time\",\n",
        "    \"cf\": \"compare\",\n",
        "    \"cia\": \"central intelligence agency\",\n",
        "    \"csl\": \"cannot stop laughing\",\n",
        "    \"cu\": \"see you\",\n",
        "    \"cul8r\": \"see you later\",\n",
        "    \"cv\": \"curriculum vitae\",\n",
        "    \"cwot\": \"complete waste of time\",\n",
        "    \"cya\": \"see you\",\n",
        "    \"cyt\": \"see you tomorrow\",\n",
        "    \"dae\": \"does anyone else\",\n",
        "    \"dbmib\": \"do not bother me i am busy\",\n",
        "    \"diy\": \"do it yourself\",\n",
        "    \"dm\": \"direct message\",\n",
        "    \"dwh\": \"during work hours\",\n",
        "    \"e123\": \"easy as one two three\",\n",
        "    \"eet\": \"eastern european time\",\n",
        "    \"eg\": \"example\",\n",
        "    \"embm\": \"early morning business meeting\",\n",
        "    \"encl\": \"enclosed\",\n",
        "    \"encl.\": \"enclosed\",\n",
        "    \"etc\": \"and so on\",\n",
        "    \"faq\": \"frequently asked questions\",\n",
        "    \"fawc\": \"for anyone who cares\",\n",
        "    \"fb\": \"facebook\",\n",
        "    \"fc\": \"fingers crossed\",\n",
        "    \"fig\": \"figure\",\n",
        "    \"fimh\": \"forever in my heart\",\n",
        "    \"ft.\": \"feet\",\n",
        "    \"ft\": \"featuring\",\n",
        "    \"ftl\": \"for the loss\",\n",
        "    \"ftw\": \"for the win\",\n",
        "    \"fwiw\": \"for what it is worth\",\n",
        "    \"fyi\": \"for your information\",\n",
        "    \"g9\": \"genius\",\n",
        "    \"gahoy\": \"get a hold of yourself\",\n",
        "    \"gal\": \"get a life\",\n",
        "    \"gcse\": \"general certificate of secondary education\",\n",
        "    \"gfn\": \"gone for now\",\n",
        "    \"gg\": \"good game\",\n",
        "    \"gl\": \"good luck\",\n",
        "    \"glhf\": \"good luck have fun\",\n",
        "    \"gmt\": \"greenwich mean time\",\n",
        "    \"gmta\": \"great minds think alike\",\n",
        "    \"gn\": \"good night\",\n",
        "    \"g.o.a.t\": \"greatest of all time\",\n",
        "    \"goat\": \"greatest of all time\",\n",
        "    \"goi\": \"get over it\",\n",
        "    \"gps\": \"global positioning system\",\n",
        "    \"gr8\": \"great\",\n",
        "    \"gratz\": \"congratulations\",\n",
        "    \"gyal\": \"girl\",\n",
        "    \"h&c\": \"hot and cold\",\n",
        "    \"hp\": \"horsepower\",\n",
        "    \"hr\": \"hour\",\n",
        "    \"hrh\": \"his royal highness\",\n",
        "    \"ht\": \"height\",\n",
        "    \"ibrb\": \"i will be right back\",\n",
        "    \"ic\": \"i see\",\n",
        "    \"icq\": \"i seek you\",\n",
        "    \"icymi\": \"in case you missed it\",\n",
        "    \"idc\": \"i do not care\",\n",
        "    \"idgadf\": \"i do not give a damn fuck\",\n",
        "    \"idgaf\": \"i do not give a fuck\",\n",
        "    \"idk\": \"i do not know\",\n",
        "    \"ie\": \"that is\",\n",
        "    \"i.e\": \"that is\",\n",
        "    \"ig\":\"i guess\",\n",
        "    \"ifyp\": \"i feel your pain\",\n",
        "    \"IG\": \"instagram\",\n",
        "    \"iirc\": \"if i remember correctly\",\n",
        "    \"ilu\": \"i love you\",\n",
        "    \"ily\": \"i love you\",\n",
        "    \"imho\": \"in my humble opinion\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"imu\": \"i miss you\",\n",
        "    \"iow\": \"in other words\",\n",
        "    \"irl\": \"in real life\",\n",
        "    \"j4f\": \"just for fun\",\n",
        "    \"jic\": \"just in case\",\n",
        "    \"jk\": \"just kidding\",\n",
        "    \"jsyk\": \"just so you know\",\n",
        "    \"l8r\": \"later\",\n",
        "    \"lb\": \"pound\",\n",
        "    \"lbs\": \"pounds\",\n",
        "    \"ldr\": \"long distance relationship\",\n",
        "    \"lmao\": \"laugh my ass off\",\n",
        "    \"lmfao\": \"laugh my fucking ass off\",\n",
        "    \"lol\": \"laughing out loud\",\n",
        "    \"ltd\": \"limited\",\n",
        "    \"ltns\": \"long time no see\",\n",
        "    \"m8\": \"mate\",\n",
        "    \"mf\": \"motherfucker\",\n",
        "    \"mfs\": \"motherfuckers\",\n",
        "    \"mfw\": \"my face when\",\n",
        "    \"mofo\": \"motherfucker\",\n",
        "    \"mph\": \"miles per hour\",\n",
        "    \"mr\": \"mister\",\n",
        "    \"mrw\": \"my reaction when\",\n",
        "    \"ms\": \"miss\",\n",
        "    \"mte\": \"my thoughts exactly\",\n",
        "    \"nagi\": \"not a good idea\",\n",
        "    \"nbc\": \"national broadcasting company\",\n",
        "    \"nbd\": \"not big deal\",\n",
        "    \"nfs\": \"not for sale\",\n",
        "    \"ngl\": \"not going to lie\",\n",
        "    \"nhs\": \"national health service\",\n",
        "    \"nrn\": \"no reply necessary\",\n",
        "    \"nsfl\": \"not safe for life\",\n",
        "    \"nsfw\": \"not safe for work\",\n",
        "    \"nth\": \"nice to have\",\n",
        "    \"nvr\": \"never\",\n",
        "    \"nyc\": \"new york city\",\n",
        "    \"oc\": \"original content\",\n",
        "    \"og\": \"original\",\n",
        "    \"ohp\": \"overhead projector\",\n",
        "    \"oic\": \"oh i see\",\n",
        "    \"omdb\": \"over my dead body\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"omw\": \"on my way\",\n",
        "    \"p.a\": \"per annum\",\n",
        "    \"plz\":\"please\",\n",
        "    \"p.m\": \"after midday\",\n",
        "    \"pm\": \"prime minister\",\n",
        "    \"poc\": \"people of color\",\n",
        "    \"pov\": \"point of view\",\n",
        "    \"pp\": \"pages\",\n",
        "    \"ppl\": \"people\",\n",
        "    \"prw\": \"parents are watching\",\n",
        "    \"ps\": \"postscript\",\n",
        "    \"pt\": \"point\",\n",
        "    \"ptb\": \"please text back\",\n",
        "    \"pto\": \"please turn over\",\n",
        "    \"qpsa\": \"what happens\",\n",
        "    \"ratchet\": \"rude\",\n",
        "    \"rbtl\": \"read between the lines\",\n",
        "    \"rlrt\": \"real life retweet\",\n",
        "    \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"roflol\": \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\": \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\": \"retweet\",\n",
        "    \"rt\":\"retweet\",\n",
        "    \"ru\": \"are you\",\n",
        "    \"shid\": \"slaps head in disgust\",\n",
        "    \"somy\": \"sick of me yet\",\n",
        "    \"ruok\": \"are you ok\",\n",
        "    \"sfw\": \"safe for work\",\n",
        "    \"sk8\": \"skate\",\n",
        "    \"smh\": \"shake my head\",\n",
        "    \"sq\": \"square\",\n",
        "    \"srsly\": \"seriously\",\n",
        "    \"ssdd\": \"same stuff different day\",\n",
        "    \"tbh\": \"to be honest\",\n",
        "    \"tbs\": \"tablespoonful\",\n",
        "    \"tbsp\": \"tablespoonful\",\n",
        "    \"tfw\": \"that feeling when\",\n",
        "    \"thks\": \"thank you\",\n",
        "    \"tho\": \"though\",\n",
        "    \"thx\": \"thank you\",\n",
        "    \"tia\": \"thanks in advance\",\n",
        "    \"til\": \"today i learned\",\n",
        "    \"tl;dr\": \"too long i did not read\",\n",
        "    \"tldr\": \"too long i did not read\",\n",
        "    \"tmb\": \"tweet me back\",\n",
        "    \"tntl\": \"trying not to laugh\",\n",
        "    \"ttyl\": \"talk to you later\",\n",
        "    \"u\": \"you\",\n",
        "    \"u2\": \"you too\",\n",
        "    \"u4e\": \"yours forever\",\n",
        "    \"utc\": \"coordinated universal time\",\n",
        "    \"w/\": \"with\",\n",
        "    \"w/o\": \"without\",\n",
        "    \"w8\": \"wait\",\n",
        "    \"wassup\": \"what is up\",\n",
        "    \"wb\": \"welcome back\",\n",
        "    \"wtf\": \"what the fuck\",\n",
        "    \"wtg\": \"way to go\",\n",
        "    \"wtpa\": \"where the party at\",\n",
        "    \"wuf\": \"where are you from\",\n",
        "    \"wuzup\": \"what is up\",\n",
        "    \"wywh\": \"wish you were here\",\n",
        "    \"yd\": \"yard\",\n",
        "    \"u\":\"you\",\n",
        "    \"ygtr\": \"you got that right\",\n",
        "    \"ynk\": \"you never know\",\n",
        "    \"zzz\": \"sleeping bored and tired\"\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_1Mtb4tu-GiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to replace abbreviations in text\n",
        "def expand_abbreviations(text):\n",
        "    words = text.split()\n",
        "    expanded_words = [abbreviations[word.lower()] if word.lower() in abbreviations else word for word in words]\n",
        "    return ' '.join(expanded_words)\n",
        "\n",
        "# Define the remove_stop_words function\n",
        "def remove_stop_words(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Function to decode HTML entities\n",
        "def decode_html_entities(text):\n",
        "    return html.unescape(text)\n",
        "\n",
        "# Example function to expand contractions\n",
        "def expand_contractions(text):\n",
        "   expanded_text = contractions.fix(text)\n",
        "   return expanded_text\n",
        "\n",
        "\n",
        "# Function to convert emojis to words\n",
        "def convert_emojis_to_words(text):\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    # Ensure there is a space before and after each emoji word\n",
        "    text = re.sub(r'(:\\w+:)', r' \\1 ', text)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "5ST_A7Ou-Q1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['tweet'] = df['tweet'].apply(convert_emojis_to_words)\n",
        "# Normalize text\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = decode_html_entities(text)\n",
        "    text = convert_emojis_to_words(text)\n",
        "    text = expand_abbreviations(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'&amp;', 'and', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(normalize_text)\n",
        "\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "df['tweet_tokens'] = df['tweet'].apply(word_tokenize)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['tweet_tokens'] = df['tweet_tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n"
      ],
      "metadata": {
        "id": "Mb6cqxdB-W1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "# Function to perform lemmatization without POS tagging\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Apply lemmatization to 'tweet_tokens' column\n",
        "df['tweet_tokens'] = df['tweet_tokens'].apply(lemmatize_tokens)"
      ],
      "metadata": {
        "id": "jqShETMi-ZVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove empty rows\n",
        "df = df[df['tweet_tokens'].str.strip().astype(bool)]\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Verify if there are any empty rows left\n",
        "print(df['tweet_tokens'].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK6PE0hI-ez0",
        "outputId": "20ff4098-a8f5-4f00-d93a-b9ffc2addd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'count' column\n",
        "df = df.drop(columns=['count'])\n",
        "\n",
        "# Verify and clean columns\n",
        "df = df[['hate_speech', 'offensive_language', 'neither', 'class', 'tweet', 'tweet_tokens']]\n",
        "\n",
        "\n",
        "\n",
        "# Show some sample data after preprocessing\n",
        "print(df.head())\n",
        "\n",
        "# Save cleaned data\n",
        "df.to_csv('cleaned_dataset_combined.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swj5VisF-jde",
        "outputId": "a5931ce6-85df-49b4-9ab7-ea44d8b65bec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   hate_speech  offensive_language  neither  class  \\\n",
            "0            0                   0        3      2   \n",
            "1            0                   3        0      1   \n",
            "2            0                   3        0      1   \n",
            "3            0                   2        1      1   \n",
            "4            0                   6        0      1   \n",
            "\n",
            "                                               tweet  \\\n",
            "0  retwet as a woman you should not complain abou...   \n",
            "1  retwet boy dats coldtyga dwn bad for cufin dat...   \n",
            "2  retwet dawg retwet you ever fuck a bitch and s...   \n",
            "3                        retwet she lok like a trany   \n",
            "4  retwet the shit you hear about me might be tru...   \n",
            "\n",
            "                                        tweet_tokens  \n",
            "0  [retwet, woman, complain, cleaning, house, man...  \n",
            "1  [retwet, boy, dat, coldtyga, dwn, bad, cufin, ...  \n",
            "2  [retwet, dawg, retwet, ever, fuck, bitch, star...  \n",
            "3                         [retwet, lok, like, trany]  \n",
            "4  [retwet, shit, hear, might, true, might, faker...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Classifier\n"
      ],
      "metadata": {
        "id": "y4p6z7lT_sLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/cleaned_dataset_combined (4).csv')\n",
        "\n",
        "# Convert string representation of list to actual list\n",
        "df['tweet_tokens'] = df['tweet_tokens'].apply(ast.literal_eval)\n",
        "\n",
        "# Split the dataset into feature (X) and target (y) variables\n",
        "X = df[['tweet_tokens']]  # Feature variable\n",
        "y = df['class']           # Target variable\n",
        "\n",
        "# Verify if 'tweet_tokens' column exists\n",
        "print(\"Columns in X:\", X.columns)\n",
        "\n",
        "# Join tokens into a single string\n",
        "X['tweet_tokens'] = X['tweet_tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use TfidfVectorizer for the 'tweet_tokens' column\n",
        "tfidf = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf.fit_transform(X_train['tweet_tokens'])\n",
        "X_test_tfidf = tfidf.transform(X_test['tweet_tokens'])\n",
        "\n",
        "# Create a pipeline with SMOTE and Random Forest, specifying hyperparameters\n",
        "pipeline = ImbPipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('smote', SMOTE(random_state=42)),\n",
        "    ('classifier', RandomForestClassifier(random_state=42,\n",
        "                                          max_depth=None,\n",
        "                                          min_samples_leaf=1,\n",
        "                                          min_samples_split=5,\n",
        "                                          n_estimators=200))\n",
        "])\n",
        "\n",
        "# Fit the pipeline on the training data\n",
        "pipeline.fit(X_train['tweet_tokens'], y_train)\n",
        "\n",
        "# Evaluate the pipeline on the test set\n",
        "accuracy = pipeline.score(X_test['tweet_tokens'], y_test)\n",
        "print(f'Test set accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = pipeline.predict(X_test['tweet_tokens'])\n",
        "\n",
        "# Generate and display the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Generate and display the confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT5ewkQUZ6OP",
        "outputId": "70c484c4-7896-4e68-beb4-0a59923b2df2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in X: Index(['tweet_tokens'], dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-c9d9e3843e2d>:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X['tweet_tokens'] = X['tweet_tokens'].apply(lambda x: ' '.join(x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set accuracy: 0.94\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.97      4144\n",
            "           1       0.81      0.87      0.84       810\n",
            "\n",
            "    accuracy                           0.94      4954\n",
            "   macro avg       0.89      0.91      0.90      4954\n",
            "weighted avg       0.95      0.94      0.95      4954\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3979  165]\n",
            " [ 109  701]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NzbmK0joaNOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}