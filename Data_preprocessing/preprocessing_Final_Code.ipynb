{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c61fc28e-dbe1-4508-9131-801a7a6bcdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\arkadip ghosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.12.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in c:\\users\\arkadip ghosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from emoji) (4.7.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\arkadip ghosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.73)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\users\\arkadip ghosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: anyascii in c:\\users\\arkadip ghosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in c:\\users\\arkadip ghosh\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!pip install contractions\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import emoji\n",
    "import html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d29709-f784-43b2-a7af-f952ed87b9e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ARKADIP\n",
      "[nltk_data]     GHOSH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\ARKADIP\n",
      "[nltk_data]     GHOSH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\ARKADIP\n",
      "[nltk_data]     GHOSH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ARKADIP GHOSH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('labeled_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b355601-1545-4484-bc87-5f3bef4604fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows and handle missing values\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d23b0b81-c560-45b8-bb71-067df3496361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary for short forms and slangs\n",
    "short_forms = {\n",
    "    \"$\": \"dollar\",\n",
    "    \"â‚¬\": \"euro\",\n",
    "    \"4ao\": \"for adults only\",\n",
    "    \"a.m\": \"before midday\",\n",
    "    \"a3\": \"anytime anywhere anyplace\",\n",
    "    \"aamof\": \"as a matter of fact\",\n",
    "    \"acct\": \"account\",\n",
    "    \"adih\": \"another day in hell\",\n",
    "    \"afaic\": \"as far as i am concerned\",\n",
    "    \"afaict\": \"as far as i can tell\",\n",
    "    \"afaik\": \"as far as i know\",\n",
    "    \"afair\": \"as far as i remember\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"app\": \"application\",\n",
    "    \"approx\": \"approximately\",\n",
    "    \"apps\": \"applications\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"asl\": \"age sex location\",\n",
    "    \"atk\": \"at the keyboard\",\n",
    "    \"ave.\": \"avenue\",\n",
    "    \"aymm\": \"are you my mother\",\n",
    "    \"ayor\": \"at your own risk\",\n",
    "    \"b&b\": \"bed and breakfast\",\n",
    "    \"b+b\": \"bed and breakfast\",\n",
    "    \"b.c\": \"before christ\",\n",
    "    \"b2b\": \"business to business\",\n",
    "    \"b2c\": \"business to customer\",\n",
    "    \"b4\": \"before\",\n",
    "    \"b4n\": \"bye for now\",\n",
    "    \"b@u\": \"back at you\",\n",
    "    \"bae\": \"before anyone else\",\n",
    "    \"bak\": \"back at keyboard\",\n",
    "    \"bbbg\": \"bye bye be good\",\n",
    "    \"bbc\": \"british broadcasting corporation\",\n",
    "    \"bbias\": \"be back in a second\",\n",
    "    \"bbl\": \"be back later\",\n",
    "    \"bbs\": \"be back soon\",\n",
    "    \"be4\": \"before\",\n",
    "    \"bfn\": \"bye for now\",\n",
    "    \"blvd\": \"boulevard\",\n",
    "    \"bout\": \"about\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"bros\": \"brothers\",\n",
    "    \"brt\": \"be right there\",\n",
    "    \"bsaaw\": \"big smile and a wink\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"bwl\": \"bursting with laughter\",\n",
    "    \"c/o\": \"care of\",\n",
    "    \"cet\": \"central european time\",\n",
    "    \"cf\": \"compare\",\n",
    "    \"cia\": \"central intelligence agency\",\n",
    "    \"csl\": \"cannot stop laughing\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cul8r\": \"see you later\",\n",
    "    \"cv\": \"curriculum vitae\",\n",
    "    \"cwot\": \"complete waste of time\",\n",
    "    \"cya\": \"see you\",\n",
    "    \"cyt\": \"see you tomorrow\",\n",
    "    \"dae\": \"does anyone else\",\n",
    "    \"dbmib\": \"do not bother me i am busy\",\n",
    "    \"diy\": \"do it yourself\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"dwh\": \"during work hours\",\n",
    "    \"e123\": \"easy as one two three\",\n",
    "    \"eet\": \"eastern european time\",\n",
    "    \"eg\": \"example\",\n",
    "    \"embm\": \"early morning business meeting\",\n",
    "    \"encl\": \"enclosed\",\n",
    "    \"encl.\": \"enclosed\",\n",
    "    \"etc\": \"and so on\",\n",
    "    \"faq\": \"frequently asked questions\",\n",
    "    \"fawc\": \"for anyone who cares\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"fc\": \"fingers crossed\",\n",
    "    \"fig\": \"figure\",\n",
    "    \"fimh\": \"forever in my heart\",\n",
    "    \"fk\" : \"fuck\",\n",
    "    \"ft.\": \"feet\",\n",
    "    \"ft\": \"featuring\",\n",
    "    \"ftl\": \"for the loss\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"fwiw\": \"for what it is worth\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"g9\": \"genius\",\n",
    "    \"gahoy\": \"get a hold of yourself\",\n",
    "    \"gal\": \"get a life\",\n",
    "    \"gcse\": \"general certificate of secondary education\",\n",
    "    \"gfn\": \"gone for now\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"gl\": \"good luck\",\n",
    "    \"glhf\": \"good luck have fun\",\n",
    "    \"gmt\": \"greenwich mean time\",\n",
    "    \"gmta\": \"great minds think alike\",\n",
    "    \"gn\": \"good night\",\n",
    "    \"g.o.a.t\": \"greatest of all time\",\n",
    "    \"goat\": \"greatest of all time\",\n",
    "    \"goi\": \"get over it\",\n",
    "    \"gps\": \"global positioning system\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"gratz\": \"congratulations\",\n",
    "    \"gyal\": \"girl\",\n",
    "    \"h&c\": \"hot and cold\",\n",
    "    \"hp\": \"horsepower\",\n",
    "    \"hr\": \"hour\",\n",
    "    \"hrh\": \"his royal highness\",\n",
    "    \"ht\": \"height\",\n",
    "    \"ibrb\": \"i will be right back\",\n",
    "    \"ic\": \"i see\",\n",
    "    \"icq\": \"i seek you\",\n",
    "    \"icymi\": \"in case you missed it\",\n",
    "    \"idc\": \"i do not care\",\n",
    "    \"idgadf\": \"i do not give a damn fuck\",\n",
    "    \"idgaf\": \"i do not give a fuck\",\n",
    "    \"idk\": \"i do not know\",\n",
    "    \"ie\": \"that is\",\n",
    "    \"i.e\": \"that is\",\n",
    "    \"ig\":\"i guess\",\n",
    "    \"ifyp\": \"i feel your pain\",\n",
    "    \"IG\": \"instagram\",\n",
    "    \"iirc\": \"if i remember correctly\",\n",
    "    \"ilu\": \"i love you\",\n",
    "    \"ily\": \"i love you\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imu\": \"i miss you\",\n",
    "    \"iow\": \"in other words\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"j4f\": \"just for fun\",\n",
    "    \"jic\": \"just in case\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"jsyk\": \"just so you know\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"lb\": \"pound\",\n",
    "    \"lbs\": \"pounds\",\n",
    "    \"ldr\": \"long distance relationship\",\n",
    "    \"lmao\": \"laugh my ass off\",\n",
    "    \"lmfao\": \"laugh my fucking ass off\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"ltd\": \"limited\",\n",
    "    \"ltns\": \"long time no see\",\n",
    "    \"m8\": \"mate\",\n",
    "    \"mf\": \"motherfucker\",\n",
    "    \"mfs\": \"motherfuckers\",\n",
    "    \"mfw\": \"my face when\",\n",
    "    \"mofo\": \"motherfucker\",\n",
    "    \"mph\": \"miles per hour\",\n",
    "    \"mr\": \"mister\",\n",
    "    \"mrw\": \"my reaction when\",\n",
    "    \"ms\": \"miss\",\n",
    "    \"mte\": \"my thoughts exactly\",\n",
    "    \"nagi\": \"not a good idea\",\n",
    "    \"nbc\": \"national broadcasting company\",\n",
    "    \"nbd\": \"not big deal\",\n",
    "    \"nfs\": \"not for sale\",\n",
    "    \"ngl\": \"not going to lie\",\n",
    "    \"nhs\": \"national health service\",\n",
    "    \"nrn\": \"no reply necessary\",\n",
    "    \"nsfl\": \"not safe for life\",\n",
    "    \"nsfw\": \"not safe for work\",\n",
    "    \"nth\": \"nice to have\",\n",
    "    \"nvr\": \"never\",\n",
    "    \"nyc\": \"new york city\",\n",
    "    \"oc\": \"original content\",\n",
    "    \"og\": \"original\",\n",
    "    \"ohp\": \"overhead projector\",\n",
    "    \"oic\": \"oh i see\",\n",
    "    \"omdb\": \"over my dead body\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"p.a\": \"per annum\",\n",
    "    \"plz\":\"please\",\n",
    "    \"p.m\": \"after midday\",\n",
    "    \"pm\": \"prime minister\",\n",
    "    \"poc\": \"people of color\",\n",
    "    \"pov\": \"point of view\",\n",
    "    \"pp\": \"pages\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"prw\": \"parents are watching\",\n",
    "    \"ps\": \"postscript\",\n",
    "    \"pt\": \"point\",\n",
    "    \"ptb\": \"please text back\",\n",
    "    \"pto\": \"please turn over\",\n",
    "    \"qpsa\": \"what happens\",  # \"que pasa\",\n",
    "    \"ratchet\": \"rude\",\n",
    "    \"rbtl\": \"read between the lines\",\n",
    "    \"rlrt\": \"real life retweet\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"roflol\": \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\": \"rolling on the floor laughing my ass off\",\n",
    "    \"rt\": \"retweet\",\n",
    "    \"ru\": \"are you\",\n",
    "    \"shid\": \"slaps head in disgust\",\n",
    "    \"somy\": \"sick of me yet\",\n",
    "    \"ruok\": \"are you ok\",\n",
    "    \"sfw\": \"safe for work\",\n",
    "    \"sk8\": \"skate\",\n",
    "    \"smh\": \"shake my head\",\n",
    "    \"sq\": \"square\",\n",
    "    \"srsly\": \"seriously\",\n",
    "    \"ssdd\": \"same stuff different day\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"tbs\": \"tablespoonful\",\n",
    "    \"tbsp\": \"tablespoonful\",\n",
    "    \"tfw\": \"that feeling when\",\n",
    "    \"thks\": \"thanks\",\n",
    "    \"tho\": \"though\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"tldr\": \"too long did not read\",\n",
    "    \"tmi\": \"too much information\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"u\": \"you\",\n",
    "    \"u2\": \"you too\",\n",
    "    \"u4e\": \"yours forever\",\n",
    "    \"wb\": \"welcome back\",\n",
    "    \"wtf\": \"what the fuck\",\n",
    "    \"wtg\": \"way to go\",\n",
    "    \"wtpa\": \"where the party at\",\n",
    "    \"wuf\": \"where are you from\",\n",
    "    \"wuzup\": \"what is up\",\n",
    "    \"wywh\": \"wish you were here\",\n",
    "    \"yolo\": \"you only live once\",\n",
    "    \"y\": \"why\",\n",
    "    \"yr\": \"your\",\n",
    "    \"yup\": \"yes\",\n",
    "    \"yvw\": \"you are welcome\",\n",
    "    \"ywt\": \"you are welcome too\",\n",
    "    \"zzzz\": \"sleeping bored and tired\"\n",
    "}\n",
    "\n",
    "# Function to replace short forms and slangs in text\n",
    "def replace_short_forms(text):\n",
    "    words = text.split()\n",
    "    replaced_words = [short_forms[word.lower()] if word.lower() in short_forms else word for word in words]\n",
    "    return ' '.join(replaced_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53ac6320-1cec-4646-8d9b-081f43a2c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words_set = set(stopwords.words('english'))\n",
    "    return [word for word in tokens if word not in stop_words_set]\n",
    "\n",
    "# Function to decode HTML entities\n",
    "def html_decode(text):\n",
    "    return html.unescape(text)\n",
    "\n",
    "# Function to expand contractions\n",
    "def expand_contrs(text):\n",
    "    return contractions.fix(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c899a272-de6a-41b1-b14d-3b21030e8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert emojis to words\n",
    "def emoji_to_words(text):\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
    "    text = re.sub(r'(:\\w+:)', r' \\1 ', text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16936844-9871-439b-97c1-57b59d8ad3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize text\n",
    "def text_normalize(text):\n",
    "    text = text.lower()\n",
    "    text = html_decode(text)\n",
    "    text = emoji_to_words(text)\n",
    "    text = replace_short_forms(text)\n",
    "    text = expand_contrs(text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'&', 'and', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2432de9b-62cb-4b58-8ad8-78f83fdb5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization\n",
    "data['tweet'] = data['tweet'].apply(text_normalize)\n",
    "\n",
    "# Tokenize tweets\n",
    "data['tokens'] = data['tweet'].apply(word_tokenize)\n",
    "\n",
    "# Remove stopwords\n",
    "data['tokens'] = data['tokens'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e84a3c-4afc-400a-b8ef-29888925dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer initialization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to perform POS tagging and lemmatization\n",
    "def lemmatize_tokens(tokens):\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    def get_wordnet_pos(tag):\n",
    "        if tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    lemmatized = []\n",
    "    for token, tag in pos_tags:\n",
    "        wn_pos = get_wordnet_pos(tag)\n",
    "        if wn_pos:\n",
    "            lemmatized.append(lemmatizer.lemmatize(token, pos=wn_pos))\n",
    "        else:\n",
    "            lemmatized.append(lemmatizer.lemmatize(token))\n",
    "    \n",
    "    return lemmatized\n",
    "\n",
    "# Apply lemmatization\n",
    "data['tokens'] = data['tokens'].apply(lemmatize_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a58c1ed-bfb2-4e72-b21f-6bccca377ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hate_speech  offensive_language  neither  class  \\\n",
      "0            0                   0        3      2   \n",
      "1            0                   3        0      1   \n",
      "2            0                   3        0      1   \n",
      "3            0                   2        1      1   \n",
      "4            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \\\n",
      "0  retwet as a woman you should not complain abou...   \n",
      "1  retwet boy dats coldtyga dwn bad for cufin dat...   \n",
      "2  retwet dawg retwet you ever fuck a bitch and s...   \n",
      "3                        retwet she lok like a trany   \n",
      "4  retwet the shit you hear about me might be tru...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [retwet, woman, complain, cleaning, house, man...  \n",
      "1  [retwet, boy, dat, coldtyga, dwn, bad, cufin, ...  \n",
      "2  [retwet, dawg, retwet, ever, fuck, bitch, star...  \n",
      "3                         [retwet, lok, like, trany]  \n",
      "4  [retwet, shit, hear, might, true, might, faker...  \n"
     ]
    }
   ],
   "source": [
    "# Drop empty rows\n",
    "data = data[data['tokens'].str.strip().astype(bool)]\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Drop unnecessary column\n",
    "data.drop(columns=['count'], inplace=True)\n",
    "\n",
    "# Reorganize columns\n",
    "data = data[['hate_speech', 'offensive_language', 'neither', 'class', 'tweet', 'tokens']]\n",
    "\n",
    "# Display sample data\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd61c33b-d4b7-4d43-8877-ae5f8462bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data\n",
    "data.to_csv('new_processed_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98187e4e-17f3-4f8f-bf91-837595d36ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
