{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbllr0KyofGw",
        "outputId": "d0d88d11-e3da-4948-ccec-0b231b4ea8d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.12.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detecting hate speech in Twitter posts**"
      ],
      "metadata": {
        "id": "xupgz6aIj7X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing\n"
      ],
      "metadata": {
        "id": "T_UZh5gWkYPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import emoji\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/labeled_dataset.csv')\n",
        "\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7OP6Sm09ptpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e39618e-8141-4fe8-a120-edf1b1200be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary for message short forms and slangs\n",
        "abbreviations = {\n",
        "    \"$\": \"dollar\",\n",
        "    \"â‚¬\": \"euro\",\n",
        "    \"4ao\": \"for adults only\",\n",
        "    \"a.m\": \"before midday\",\n",
        "    \"a3\": \"anytime anywhere anyplace\",\n",
        "    \"aamof\": \"as a matter of fact\",\n",
        "    \"acct\": \"account\",\n",
        "    \"adih\": \"another day in hell\",\n",
        "    \"afaic\": \"as far as i am concerned\",\n",
        "    \"afaict\": \"as far as i can tell\",\n",
        "    \"afaik\": \"as far as i know\",\n",
        "    \"afair\": \"as far as i remember\",\n",
        "    \"afk\": \"away from keyboard\",\n",
        "    \"app\": \"application\",\n",
        "    \"approx\": \"approximately\",\n",
        "    \"apps\": \"applications\",\n",
        "    \"asap\": \"as soon as possible\",\n",
        "    \"asl\": \"age sex location\",\n",
        "    \"atk\": \"at the keyboard\",\n",
        "    \"ave.\": \"avenue\",\n",
        "    \"aymm\": \"are you my mother\",\n",
        "    \"ayor\": \"at your own risk\",\n",
        "    \"b&b\": \"bed and breakfast\",\n",
        "    \"b+b\": \"bed and breakfast\",\n",
        "    \"b.c\": \"before christ\",\n",
        "    \"b2b\": \"business to business\",\n",
        "    \"b2c\": \"business to customer\",\n",
        "    \"b4\": \"before\",\n",
        "    \"b4n\": \"bye for now\",\n",
        "    \"b@u\": \"back at you\",\n",
        "    \"bae\": \"before anyone else\",\n",
        "    \"bak\": \"back at keyboard\",\n",
        "    \"bbbg\": \"bye bye be good\",\n",
        "    \"bbc\": \"british broadcasting corporation\",\n",
        "    \"bbias\": \"be back in a second\",\n",
        "    \"bbl\": \"be back later\",\n",
        "    \"bbs\": \"be back soon\",\n",
        "    \"be4\": \"before\",\n",
        "    \"bfn\": \"bye for now\",\n",
        "    \"blvd\": \"boulevard\",\n",
        "    \"bout\": \"about\",\n",
        "    \"brb\": \"be right back\",\n",
        "    \"bros\": \"brothers\",\n",
        "    \"brt\": \"be right there\",\n",
        "    \"bsaaw\": \"big smile and a wink\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"bwl\": \"bursting with laughter\",\n",
        "    \"c/o\": \"care of\",\n",
        "    \"cet\": \"central european time\",\n",
        "    \"cf\": \"compare\",\n",
        "    \"cia\": \"central intelligence agency\",\n",
        "    \"csl\": \"cannot stop laughing\",\n",
        "    \"cu\": \"see you\",\n",
        "    \"cul8r\": \"see you later\",\n",
        "    \"cv\": \"curriculum vitae\",\n",
        "    \"cwot\": \"complete waste of time\",\n",
        "    \"cya\": \"see you\",\n",
        "    \"cyt\": \"see you tomorrow\",\n",
        "    \"dae\": \"does anyone else\",\n",
        "    \"dbmib\": \"do not bother me i am busy\",\n",
        "    \"diy\": \"do it yourself\",\n",
        "    \"dm\": \"direct message\",\n",
        "    \"dwh\": \"during work hours\",\n",
        "    \"e123\": \"easy as one two three\",\n",
        "    \"eet\": \"eastern european time\",\n",
        "    \"eg\": \"example\",\n",
        "    \"embm\": \"early morning business meeting\",\n",
        "    \"encl\": \"enclosed\",\n",
        "    \"encl.\": \"enclosed\",\n",
        "    \"etc\": \"and so on\",\n",
        "    \"faq\": \"frequently asked questions\",\n",
        "    \"fawc\": \"for anyone who cares\",\n",
        "    \"fb\": \"facebook\",\n",
        "    \"fc\": \"fingers crossed\",\n",
        "    \"fig\": \"figure\",\n",
        "    \"fimh\": \"forever in my heart\",\n",
        "    \"ft.\": \"feet\",\n",
        "    \"ft\": \"featuring\",\n",
        "    \"ftl\": \"for the loss\",\n",
        "    \"ftw\": \"for the win\",\n",
        "    \"fwiw\": \"for what it is worth\",\n",
        "    \"fyi\": \"for your information\",\n",
        "    \"g9\": \"genius\",\n",
        "    \"gahoy\": \"get a hold of yourself\",\n",
        "    \"gal\": \"get a life\",\n",
        "    \"gcse\": \"general certificate of secondary education\",\n",
        "    \"gfn\": \"gone for now\",\n",
        "    \"gg\": \"good game\",\n",
        "    \"gl\": \"good luck\",\n",
        "    \"glhf\": \"good luck have fun\",\n",
        "    \"gmt\": \"greenwich mean time\",\n",
        "    \"gmta\": \"great minds think alike\",\n",
        "    \"gn\": \"good night\",\n",
        "    \"g.o.a.t\": \"greatest of all time\",\n",
        "    \"goat\": \"greatest of all time\",\n",
        "    \"goi\": \"get over it\",\n",
        "    \"gps\": \"global positioning system\",\n",
        "    \"gr8\": \"great\",\n",
        "    \"gratz\": \"congratulations\",\n",
        "    \"gyal\": \"girl\",\n",
        "    \"h&c\": \"hot and cold\",\n",
        "    \"hp\": \"horsepower\",\n",
        "    \"hr\": \"hour\",\n",
        "    \"hrh\": \"his royal highness\",\n",
        "    \"ht\": \"height\",\n",
        "    \"ibrb\": \"i will be right back\",\n",
        "    \"ic\": \"i see\",\n",
        "    \"icq\": \"i seek you\",\n",
        "    \"icymi\": \"in case you missed it\",\n",
        "    \"idc\": \"i do not care\",\n",
        "    \"idgadf\": \"i do not give a damn fuck\",\n",
        "    \"idgaf\": \"i do not give a fuck\",\n",
        "    \"idk\": \"i do not know\",\n",
        "    \"ie\": \"that is\",\n",
        "    \"i.e\": \"that is\",\n",
        "    \"ig\":\"i guess\",\n",
        "    \"ifyp\": \"i feel your pain\",\n",
        "    \"IG\": \"instagram\",\n",
        "    \"iirc\": \"if i remember correctly\",\n",
        "    \"ilu\": \"i love you\",\n",
        "    \"ily\": \"i love you\",\n",
        "    \"imho\": \"in my humble opinion\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"imu\": \"i miss you\",\n",
        "    \"iow\": \"in other words\",\n",
        "    \"irl\": \"in real life\",\n",
        "    \"j4f\": \"just for fun\",\n",
        "    \"jic\": \"just in case\",\n",
        "    \"jk\": \"just kidding\",\n",
        "    \"jsyk\": \"just so you know\",\n",
        "    \"l8r\": \"later\",\n",
        "    \"lb\": \"pound\",\n",
        "    \"lbs\": \"pounds\",\n",
        "    \"ldr\": \"long distance relationship\",\n",
        "    \"lmao\": \"laugh my ass off\",\n",
        "    \"lmfao\": \"laugh my fucking ass off\",\n",
        "    \"lol\": \"laughing out loud\",\n",
        "    \"ltd\": \"limited\",\n",
        "    \"ltns\": \"long time no see\",\n",
        "    \"m8\": \"mate\",\n",
        "    \"mf\": \"motherfucker\",\n",
        "    \"mfs\": \"motherfuckers\",\n",
        "    \"mfw\": \"my face when\",\n",
        "    \"mofo\": \"motherfucker\",\n",
        "    \"mph\": \"miles per hour\",\n",
        "    \"mr\": \"mister\",\n",
        "    \"mrw\": \"my reaction when\",\n",
        "    \"ms\": \"miss\",\n",
        "    \"mte\": \"my thoughts exactly\",\n",
        "    \"nagi\": \"not a good idea\",\n",
        "    \"nbc\": \"national broadcasting company\",\n",
        "    \"nbd\": \"not big deal\",\n",
        "    \"nfs\": \"not for sale\",\n",
        "    \"ngl\": \"not going to lie\",\n",
        "    \"nhs\": \"national health service\",\n",
        "    \"nrn\": \"no reply necessary\",\n",
        "    \"nsfl\": \"not safe for life\",\n",
        "    \"nsfw\": \"not safe for work\",\n",
        "    \"nth\": \"nice to have\",\n",
        "    \"nvr\": \"never\",\n",
        "    \"nyc\": \"new york city\",\n",
        "    \"oc\": \"original content\",\n",
        "    \"og\": \"original\",\n",
        "    \"ohp\": \"overhead projector\",\n",
        "    \"oic\": \"oh i see\",\n",
        "    \"omdb\": \"over my dead body\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"omw\": \"on my way\",\n",
        "    \"p.a\": \"per annum\",\n",
        "    \"plz\":\"please\",\n",
        "    \"p.m\": \"after midday\",\n",
        "    \"pm\": \"prime minister\",\n",
        "    \"poc\": \"people of color\",\n",
        "    \"pov\": \"point of view\",\n",
        "    \"pp\": \"pages\",\n",
        "    \"ppl\": \"people\",\n",
        "    \"prw\": \"parents are watching\",\n",
        "    \"ps\": \"postscript\",\n",
        "    \"pt\": \"point\",\n",
        "    \"ptb\": \"please text back\",\n",
        "    \"pto\": \"please turn over\",\n",
        "    \"qpsa\": \"what happens\",  # \"que pasa\",\n",
        "    \"ratchet\": \"rude\",\n",
        "    \"rbtl\": \"read between the lines\",\n",
        "    \"rlrt\": \"real life retweet\",\n",
        "    \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"roflol\": \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\": \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\": \"retweet\",\n",
        "    \"rt\":\"retweet\",\n",
        "    \"ru\": \"are you\",\n",
        "    \"shid\": \"slaps head in disgust\",\n",
        "    \"somy\": \"sick of me yet\",\n",
        "    \"ruok\": \"are you ok\",\n",
        "    \"sfw\": \"safe for work\",\n",
        "    \"sk8\": \"skate\",\n",
        "    \"smh\": \"shake my head\",\n",
        "    \"sq\": \"square\",\n",
        "    \"srsly\": \"seriously\",\n",
        "    \"ssdd\": \"same stuff different day\",\n",
        "    \"tbh\": \"to be honest\",\n",
        "    \"tbs\": \"tablespoonful\",\n",
        "    \"tbsp\": \"tablespoonful\",\n",
        "    \"tfw\": \"that feeling when\",\n",
        "    \"thks\": \"thank you\",\n",
        "    \"tho\": \"though\",\n",
        "    \"thx\": \"thank you\",\n",
        "    \"tia\": \"thanks in advance\",\n",
        "    \"til\": \"today i learned\",\n",
        "    \"tl;dr\": \"too long i did not read\",\n",
        "    \"tldr\": \"too long i did not read\",\n",
        "    \"tmb\": \"tweet me back\",\n",
        "    \"tntl\": \"trying not to laugh\",\n",
        "    \"ttyl\": \"talk to you later\",\n",
        "    \"u\": \"you\",\n",
        "    \"u2\": \"you too\",\n",
        "    \"u4e\": \"yours forever\",\n",
        "    \"utc\": \"coordinated universal time\",\n",
        "    \"w/\": \"with\",\n",
        "    \"w/o\": \"without\",\n",
        "    \"w8\": \"wait\",\n",
        "    \"wassup\": \"what is up\",\n",
        "    \"wb\": \"welcome back\",\n",
        "    \"wtf\": \"what the fuck\",\n",
        "    \"wtg\": \"way to go\",\n",
        "    \"wtpa\": \"where the party at\",\n",
        "    \"wuf\": \"where are you from\",\n",
        "    \"wuzup\": \"what is up\",\n",
        "    \"wywh\": \"wish you were here\",\n",
        "    \"yd\": \"yard\",\n",
        "    \"u\":\"you\",\n",
        "    \"ygtr\": \"you got that right\",\n",
        "    \"ynk\": \"you never know\",\n",
        "    \"zzz\": \"sleeping bored and tired\"\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vc85DIUzql9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to replace abbreviations in text\n",
        "def expand_abbreviations(text):\n",
        "    words = text.split()\n",
        "    expanded_words = [abbreviations[word.lower()] if word.lower() in abbreviations else word for word in words]\n",
        "    return ' '.join(expanded_words)\n",
        "\n",
        "# Define the remove_stop_words function\n",
        "def remove_stop_words(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Function to decode HTML entities\n",
        "def decode_html_entities(text):\n",
        "    return html.unescape(text)\n",
        "\n",
        "# Example function to expand contractions\n",
        "def expand_contractions(text):\n",
        "   expanded_text = contractions.fix(text)\n",
        "   return expanded_text\n",
        "\n",
        "\n",
        "# Function to convert emojis to words\n",
        "def convert_emojis_to_words(text):\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    # Ensure there is a space before and after each emoji word\n",
        "    text = re.sub(r'(:\\w+:)', r' \\1 ', text)\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "gUpsLblzq5rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import html"
      ],
      "metadata": {
        "id": "vlJp-FMsCEbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df['tweet'] = df['tweet'].apply(convert_emojis_to_words)\n",
        "# Normalize text\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = decode_html_entities(text)\n",
        "    text = convert_emojis_to_words(text)\n",
        "    text = expand_abbreviations(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'&amp;', 'and', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'(.)\\1+', r'\\1', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(normalize_text)\n",
        "\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "df['tweet_tokens'] = df['tweet'].apply(word_tokenize)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['tweet_tokens'] = df['tweet_tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n"
      ],
      "metadata": {
        "id": "2CT-nW6ZrAqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqdYVNYUabq_",
        "outputId": "cb83955a-618c-4220-af0f-4ef6bbcf66cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to perform POS tagging and lemmatization\n",
        "def lemmatize_with_pos(tokens):\n",
        "    # Perform POS tagging\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    # Map POS tags to WordNet POS tags\n",
        "    def get_wordnet_pos(treebank_tag):\n",
        "        if treebank_tag.startswith('J'):\n",
        "            return wordnet.ADJ\n",
        "        elif treebank_tag.startswith('V'):\n",
        "            return wordnet.VERB\n",
        "        elif treebank_tag.startswith('N'):\n",
        "            return wordnet.NOUN\n",
        "        elif treebank_tag.startswith('R'):\n",
        "            return wordnet.ADV\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Lemmatize tokens with POS tags\n",
        "    lemmatized_tokens = []\n",
        "    for token, pos_tag in pos_tags:\n",
        "        wordnet_pos = get_wordnet_pos(pos_tag)\n",
        "        if wordnet_pos is not None:\n",
        "            lemmatized_token = lemmatizer.lemmatize(token, pos=wordnet_pos)\n",
        "        else:\n",
        "            # If POS tag is not recognized, lemmatize without specifying POS\n",
        "            lemmatized_token = lemmatizer.lemmatize(token)\n",
        "        lemmatized_tokens.append(lemmatized_token)\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Apply lemmatization with POS tagging to 'tweet_tokens' column\n",
        "df['tweet_tokens'] = df['tweet_tokens'].apply(lemmatize_with_pos)\n"
      ],
      "metadata": {
        "id": "kdp8ihmhaIeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove empty rows\n",
        "df = df[df['tweet_tokens'].str.strip().astype(bool)]\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Verify if there are any empty rows left\n",
        "print(df['tweet_tokens'].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iajw6moWtOrU",
        "outputId": "73d564fa-5cba-4f50-f06a-15508aebb96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Drop the 'count' column\n",
        "df = df.drop(columns=['count'])\n",
        "\n",
        "# Verify and clean columns\n",
        "df = df[['hate_speech', 'offensive_language', 'neither', 'class', 'tweet', 'tweet_tokens']]\n",
        "\n",
        "\n",
        "\n",
        "# Show some sample data after preprocessing\n",
        "print(df.head())\n",
        "\n",
        "# Save cleaned data\n",
        "df.to_csv('cleaned_dataset_combined.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4iibe86rI8-",
        "outputId": "bfc0cda6-d3da-4870-84f3-8c503e173b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   hate_speech  offensive_language  neither  class  \\\n",
            "0            0                   0        3      2   \n",
            "1            0                   3        0      1   \n",
            "2            0                   3        0      1   \n",
            "3            0                   2        1      1   \n",
            "4            0                   6        0      1   \n",
            "\n",
            "                                               tweet  \\\n",
            "0  retwet as a woman you should not complain abou...   \n",
            "1  retwet boy dats coldtyga dwn bad for cufin dat...   \n",
            "2  retwet dawg retwet you ever fuck a bitch and s...   \n",
            "3                        retwet she lok like a trany   \n",
            "4  retwet the shit you hear about me might be tru...   \n",
            "\n",
            "                                        tweet_tokens  \n",
            "0  [retwet, woman, complain, cleaning, house, man...  \n",
            "1  [retwet, boy, dat, coldtyga, dwn, bad, cufin, ...  \n",
            "2  [retwet, dawg, retwet, ever, fuck, bitch, star...  \n",
            "3                         [retwet, lok, like, trany]  \n",
            "4  [retwet, shit, hear, might, true, might, faker...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pxPEfLAaBj_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Puzz-65jBgdu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}