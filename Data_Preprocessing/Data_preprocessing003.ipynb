{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Detecting hate speech in Twitter posts**"
      ],
      "metadata": {
        "id": "xupgz6aIj7X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing\n"
      ],
      "metadata": {
        "id": "T_UZh5gWkYPF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3pIAPBtcT9M",
        "outputId": "4c2ac8c1-965e-4905-d325-eaa402debdb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import html\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('labeled_dataset.csv')\n",
        "print(df.head())\n",
        "# Remove duplicates\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna()\n",
        "\n",
        "df = df.drop(columns=['count'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H3DYHf4keR1",
        "outputId": "90690562-5144-44e9-bd09-6382262e4493"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
            "0           0      3            0                   0        3      2   \n",
            "1           1      3            0                   3        0      1   \n",
            "2           2      3            0                   3        0      1   \n",
            "3           3      3            0                   2        1      1   \n",
            "4           4      6            0                   6        0      1   \n",
            "\n",
            "                                               tweet  \n",
            "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
            "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
            "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
            "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
            "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary for message short forms and slangs\n",
        "abbreviations = {\n",
        "    \"$\": \"dollar\",\n",
        "    \"â‚¬\": \"euro\",\n",
        "    \"4ao\": \"for adults only\",\n",
        "    \"a.m\": \"before midday\",\n",
        "    \"a3\": \"anytime anywhere anyplace\",\n",
        "    \"aamof\": \"as a matter of fact\",\n",
        "    \"acct\": \"account\",\n",
        "    \"adih\": \"another day in hell\",\n",
        "    \"afaic\": \"as far as i am concerned\",\n",
        "    \"afaict\": \"as far as i can tell\",\n",
        "    \"afaik\": \"as far as i know\",\n",
        "    \"afair\": \"as far as i remember\",\n",
        "    \"afk\": \"away from keyboard\",\n",
        "    \"app\": \"application\",\n",
        "    \"approx\": \"approximately\",\n",
        "    \"apps\": \"applications\",\n",
        "    \"asap\": \"as soon as possible\",\n",
        "    \"asl\": \"age sex location\",\n",
        "    \"atk\": \"at the keyboard\",\n",
        "    \"ave.\": \"avenue\",\n",
        "    \"aymm\": \"are you my mother\",\n",
        "    \"ayor\": \"at your own risk\",\n",
        "    \"b&b\": \"bed and breakfast\",\n",
        "    \"b+b\": \"bed and breakfast\",\n",
        "    \"b.c\": \"before christ\",\n",
        "    \"b2b\": \"business to business\",\n",
        "    \"b2c\": \"business to customer\",\n",
        "    \"b4\": \"before\",\n",
        "    \"b4n\": \"bye for now\",\n",
        "    \"b@u\": \"back at you\",\n",
        "    \"bae\": \"before anyone else\",\n",
        "    \"bak\": \"back at keyboard\",\n",
        "    \"bbbg\": \"bye bye be good\",\n",
        "    \"bbc\": \"british broadcasting corporation\",\n",
        "    \"bbias\": \"be back in a second\",\n",
        "    \"bbl\": \"be back later\",\n",
        "    \"bbs\": \"be back soon\",\n",
        "    \"be4\": \"before\",\n",
        "    \"bfn\": \"bye for now\",\n",
        "    \"blvd\": \"boulevard\",\n",
        "    \"bout\": \"about\",\n",
        "    \"brb\": \"be right back\",\n",
        "    \"bros\": \"brothers\",\n",
        "    \"brt\": \"be right there\",\n",
        "    \"bsaaw\": \"big smile and a wink\",\n",
        "    \"btw\": \"by the way\",\n",
        "    \"bwl\": \"bursting with laughter\",\n",
        "    \"c/o\": \"care of\",\n",
        "    \"cet\": \"central european time\",\n",
        "    \"cf\": \"compare\",\n",
        "    \"cia\": \"central intelligence agency\",\n",
        "    \"csl\": \"cannot stop laughing\",\n",
        "    \"cu\": \"see you\",\n",
        "    \"cul8r\": \"see you later\",\n",
        "    \"cv\": \"curriculum vitae\",\n",
        "    \"cwot\": \"complete waste of time\",\n",
        "    \"cya\": \"see you\",\n",
        "    \"cyt\": \"see you tomorrow\",\n",
        "    \"dae\": \"does anyone else\",\n",
        "    \"dbmib\": \"do not bother me i am busy\",\n",
        "    \"diy\": \"do it yourself\",\n",
        "    \"dm\": \"direct message\",\n",
        "    \"dwh\": \"during work hours\",\n",
        "    \"e123\": \"easy as one two three\",\n",
        "    \"eet\": \"eastern european time\",\n",
        "    \"eg\": \"example\",\n",
        "    \"embm\": \"early morning business meeting\",\n",
        "    \"encl\": \"enclosed\",\n",
        "    \"encl.\": \"enclosed\",\n",
        "    \"etc\": \"and so on\",\n",
        "    \"faq\": \"frequently asked questions\",\n",
        "    \"fawc\": \"for anyone who cares\",\n",
        "    \"fb\": \"facebook\",\n",
        "    \"fc\": \"fingers crossed\",\n",
        "    \"fig\": \"figure\",\n",
        "    \"fimh\": \"forever in my heart\",\n",
        "    \"ft.\": \"feet\",\n",
        "    \"ft\": \"featuring\",\n",
        "    \"ftl\": \"for the loss\",\n",
        "    \"ftw\": \"for the win\",\n",
        "    \"fwiw\": \"for what it is worth\",\n",
        "    \"fyi\": \"for your information\",\n",
        "    \"g9\": \"genius\",\n",
        "    \"gahoy\": \"get a hold of yourself\",\n",
        "    \"gal\": \"get a life\",\n",
        "    \"gcse\": \"general certificate of secondary education\",\n",
        "    \"gfn\": \"gone for now\",\n",
        "    \"gg\": \"good game\",\n",
        "    \"gl\": \"good luck\",\n",
        "    \"glhf\": \"good luck have fun\",\n",
        "    \"gmt\": \"greenwich mean time\",\n",
        "    \"gmta\": \"great minds think alike\",\n",
        "    \"gn\": \"good night\",\n",
        "    \"g.o.a.t\": \"greatest of all time\",\n",
        "    \"goat\": \"greatest of all time\",\n",
        "    \"goi\": \"get over it\",\n",
        "    \"gps\": \"global positioning system\",\n",
        "    \"gr8\": \"great\",\n",
        "    \"gratz\": \"congratulations\",\n",
        "    \"gyal\": \"girl\",\n",
        "    \"h&c\": \"hot and cold\",\n",
        "    \"hp\": \"horsepower\",\n",
        "    \"hr\": \"hour\",\n",
        "    \"hrh\": \"his royal highness\",\n",
        "    \"ht\": \"height\",\n",
        "    \"ibrb\": \"i will be right back\",\n",
        "    \"ic\": \"i see\",\n",
        "    \"icq\": \"i seek you\",\n",
        "    \"icymi\": \"in case you missed it\",\n",
        "    \"idc\": \"i do not care\",\n",
        "    \"idgadf\": \"i do not give a damn fuck\",\n",
        "    \"idgaf\": \"i do not give a fuck\",\n",
        "    \"idk\": \"i do not know\",\n",
        "    \"ie\": \"that is\",\n",
        "    \"i.e\": \"that is\",\n",
        "    \"ifyp\": \"i feel your pain\",\n",
        "    \"IG\": \"instagram\",\n",
        "    \"iirc\": \"if i remember correctly\",\n",
        "    \"ilu\": \"i love you\",\n",
        "    \"ily\": \"i love you\",\n",
        "    \"imho\": \"in my humble opinion\",\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"imu\": \"i miss you\",\n",
        "    \"iow\": \"in other words\",\n",
        "    \"irl\": \"in real life\",\n",
        "    \"j4f\": \"just for fun\",\n",
        "    \"jic\": \"just in case\",\n",
        "    \"jk\": \"just kidding\",\n",
        "    \"jsyk\": \"just so you know\",\n",
        "    \"l8r\": \"later\",\n",
        "    \"lb\": \"pound\",\n",
        "    \"lbs\": \"pounds\",\n",
        "    \"ldr\": \"long distance relationship\",\n",
        "    \"lmao\": \"laugh my ass off\",\n",
        "    \"lmfao\": \"laugh my fucking ass off\",\n",
        "    \"lol\": \"laughing out loud\",\n",
        "    \"ltd\": \"limited\",\n",
        "    \"ltns\": \"long time no see\",\n",
        "    \"m8\": \"mate\",\n",
        "    \"mf\": \"motherfucker\",\n",
        "    \"mfs\": \"motherfuckers\",\n",
        "    \"mfw\": \"my face when\",\n",
        "    \"mofo\": \"motherfucker\",\n",
        "    \"mph\": \"miles per hour\",\n",
        "    \"mr\": \"mister\",\n",
        "    \"mrw\": \"my reaction when\",\n",
        "    \"ms\": \"miss\",\n",
        "    \"mte\": \"my thoughts exactly\",\n",
        "    \"nagi\": \"not a good idea\",\n",
        "    \"nbc\": \"national broadcasting company\",\n",
        "    \"nbd\": \"not big deal\",\n",
        "    \"nfs\": \"not for sale\",\n",
        "    \"ngl\": \"not going to lie\",\n",
        "    \"nhs\": \"national health service\",\n",
        "    \"nrn\": \"no reply necessary\",\n",
        "    \"nsfl\": \"not safe for life\",\n",
        "    \"nsfw\": \"not safe for work\",\n",
        "    \"nth\": \"nice to have\",\n",
        "    \"nvr\": \"never\",\n",
        "    \"nyc\": \"new york city\",\n",
        "    \"oc\": \"original content\",\n",
        "    \"og\": \"original\",\n",
        "    \"ohp\": \"overhead projector\",\n",
        "    \"oic\": \"oh i see\",\n",
        "    \"omdb\": \"over my dead body\",\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"omw\": \"on my way\",\n",
        "    \"p.a\": \"per annum\",\n",
        "    \"plz\":\"please\",\n",
        "    \"p.m\": \"after midday\",\n",
        "    \"pm\": \"prime minister\",\n",
        "    \"poc\": \"people of color\",\n",
        "    \"pov\": \"point of view\",\n",
        "    \"pp\": \"pages\",\n",
        "    \"ppl\": \"people\",\n",
        "    \"prw\": \"parents are watching\",\n",
        "    \"ps\": \"postscript\",\n",
        "    \"pt\": \"point\",\n",
        "    \"ptb\": \"please text back\",\n",
        "    \"pto\": \"please turn over\",\n",
        "    \"qpsa\": \"what happens\",  # \"que pasa\",\n",
        "    \"ratchet\": \"rude\",\n",
        "    \"rbtl\": \"read between the lines\",\n",
        "    \"rlrt\": \"real life retweet\",\n",
        "    \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"roflol\": \"rolling on the floor laughing out loud\",\n",
        "    \"rotflmao\": \"rolling on the floor laughing my ass off\",\n",
        "    \"rt\": \"retweet\",\n",
        "    \"rt\":\"retweet\",\n",
        "    \"ru\": \"are you\",\n",
        "    \"shid\": \"slaps head in disgust\",\n",
        "    \"somy\": \"sick of me yet\",\n",
        "    \"ruok\": \"are you ok\",\n",
        "    \"sfw\": \"safe for work\",\n",
        "    \"sk8\": \"skate\",\n",
        "    \"smh\": \"shake my head\",\n",
        "    \"sq\": \"square\",\n",
        "    \"srsly\": \"seriously\",\n",
        "    \"ssdd\": \"same stuff different day\",\n",
        "    \"tbh\": \"to be honest\",\n",
        "    \"tbs\": \"tablespoonful\",\n",
        "    \"tbsp\": \"tablespoonful\",\n",
        "    \"tfw\": \"that feeling when\",\n",
        "    \"thks\": \"thank you\",\n",
        "    \"tho\": \"though\",\n",
        "    \"thx\": \"thank you\",\n",
        "    \"tia\": \"thanks in advance\",\n",
        "    \"til\": \"today i learned\",\n",
        "    \"tl;dr\": \"too long i did not read\",\n",
        "    \"tldr\": \"too long i did not read\",\n",
        "    \"tmb\": \"tweet me back\",\n",
        "    \"tntl\": \"trying not to laugh\",\n",
        "    \"ttyl\": \"talk to you later\",\n",
        "    \"u\": \"you\",\n",
        "    \"u2\": \"you too\",\n",
        "    \"u4e\": \"yours forever\",\n",
        "    \"utc\": \"coordinated universal time\",\n",
        "    \"w/\": \"with\",\n",
        "    \"w/o\": \"without\",\n",
        "    \"w8\": \"wait\",\n",
        "    \"wassup\": \"what is up\",\n",
        "    \"wb\": \"welcome back\",\n",
        "    \"wtf\": \"what the fuck\",\n",
        "    \"wtg\": \"way to go\",\n",
        "    \"wtpa\": \"where the party at\",\n",
        "    \"wuf\": \"where are you from\",\n",
        "    \"wuzup\": \"what is up\",\n",
        "    \"wywh\": \"wish you were here\",\n",
        "    \"yd\": \"yard\",\n",
        "    \"ygtr\": \"you got that right\",\n",
        "    \"ynk\": \"you never know\",\n",
        "    \"zzz\": \"sleeping bored and tired\"\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2ddMdNppbOUE"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to replace abbreviations in text\n",
        "def expand_abbreviations(text):\n",
        "    words = text.split()\n",
        "    expanded_words = [abbreviations[word.lower()] if word.lower() in abbreviations else word for word in words]\n",
        "    return ' '.join(expanded_words)\n",
        "\n",
        "# Define the remove_stop_words function\n",
        "def remove_stop_words(tokens):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Function to decode HTML entities\n",
        "def decode_html_entities(text):\n",
        "    return html.unescape(text)\n",
        "\n",
        "# Function to convert emojis to words\n",
        "def convert_emojis_to_words(text):\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    # Ensure there is a space before and after each emoji word\n",
        "    text = re.sub(r'(:\\w+:)', r' \\1 ', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "2fbcSLVF5Io1"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize text\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = decode_html_entities(text)\n",
        "    text = convert_emojis_to_words(text)\n",
        "    text = expand_abbreviations(text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'&amp;', 'and', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "l1xUJdH3cv6v"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet'] = df['tweet'].apply(normalize_text)\n",
        "\n",
        "# Tokenization\n",
        "df['tweet_tokens'] = df['tweet'].apply(word_tokenize)\n",
        "\n",
        "# Remove stopwords\n",
        "df['tweet_tokens'] = df['tweet_tokens'].apply(remove_stop_words)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['tweet_tokens'] = df['tweet_tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
      ],
      "metadata": {
        "id": "X87RbA5X2Djg"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove empty rows\n",
        "df = df[df['tweet'].str.strip().astype(bool)]\n",
        "\n",
        "# Handle missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Verify if there are any empty rows left\n",
        "print(df['tweet'].isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uglxyfsVeC1v",
        "outputId": "2ac51bf9-2562-4652-a246-95bdbee50bfb"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify and clean columns\n",
        "df = df[['hate_speech', 'offensive_language', 'neither', 'class', 'tweet']]\n",
        "print(df.head())\n",
        "# Save cleaned data\n",
        "df.to_csv('cleaned_dataset.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG_4O5j1fPWv",
        "outputId": "c8163fb4-ddbd-4565-e165-7c8d3ddf4caf"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   hate_speech  offensive_language  neither  class  \\\n",
            "0            0                   0        3      2   \n",
            "1            0                   3        0      1   \n",
            "2            0                   3        0      1   \n",
            "3            0                   2        1      1   \n",
            "4            0                   6        0      1   \n",
            "\n",
            "                                               tweet  \n",
            "0  retweet as a woman you shouldnt complain about...  \n",
            "1  retweet boy dats coldtyga dwn bad for cuffin d...  \n",
            "2  retweet dawg retweet you ever fuck a bitch and...  \n",
            "3                     retweet she look like a tranny  \n",
            "4  retweet the shit you hear about me might be tr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = decode_html_entities(text)\n",
        "    text = convert_emojis_to_words(text)\n",
        "    text = expand_abbreviations(text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'&amp;', 'and', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = remove_stop_words(tokens)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "D8KVI5bkzMan"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sen=\"&#8220;@CauseWereGuys: One wrong move and consider your bitch mine http://t.co/01VJZqnaHh&#8221; @its_gabbyElla @Lyssa_Rae16\"\n",
        "print(preprocess_text(sen))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og6BW3hF89qt",
        "outputId": "ac667f0e-66f9-45a9-98aa-439f92014191"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['one', 'wrong', 'move', 'consider', 'bitch', 'mine']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UMA_HVR49IAm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}